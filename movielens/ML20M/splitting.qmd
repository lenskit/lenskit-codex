---
title: ML20M Data Splitting
order: 1
deps:
- ratings.duckdb
---

This page describes the analysis done to select the cutoffs for temporally-splitting the ML20M data set.

```{python}
from datetime import date, datetime
import plotnine as pn
```

```{python}
import duckdb
db = duckdb.connect('ratings.duckdb', read_only=True)
```

## Split Windows

Following [@mengExploringDataSplitting2020], we are going to prepare a global temporal split of the rating data.  We will target approximately a 70/15/15 train/tune/test split, but round the timestamps so our test splits are at clean calendar dates.
Searching for quantiles will get us this.

```{python}
db.query('''
SELECT quantile_cont(timestamp, 0.7) AS t_tune,
    quantile_cont(timestamp, 0.85) AS t_test,
FROM ratings
''').df()
```

This suggests that 2008-2010 (valid) and 2011-end (test) are reasonable splits.

```{python}
t_tune = '2008-01-01'
t_test = '2011-01-01'
```

```{python}
db.query(f'''
SELECT
    CASE WHEN timestamp < '{t_tune}' THEN 'train'
         WHEN timestamp < '{t_test}' THEN 'tune'
         ELSE 'test'
    END AS part,
    count(*) AS n_ratings,
    count(distinct user_id) AS n_users,
    count(distinct item_id) AS n_items
FROM ratings
GROUP BY part
''').df()
```

How many test users have at least 5 training ratings?

```{python}
db.query(f'''
SELECT COUNT(DISTINCT user_id) n_users, COUNT(*) n_ratings
FROM ratings
WHERE user_id IN (
    SELECT DISTINCT user_id FROM ratings
    WHERE timestamp < '{t_test}'GROUP BY user_id
    HAVING count(*) >= 5
)
AND timestamp >= '{t_test}'
''').df()
```

And for tuning?

```{python}
db.query(f'''
SELECT COUNT(DISTINCT user_id) n_users, COUNT(*) n_ratings
FROM ratings
WHERE user_id IN (
    SELECT DISTINCT user_id FROM ratings
    WHERE timestamp < '{t_tune}'
    GROUP BY user_id
    HAVING count(*) >= 5
)
AND timestamp >= '{t_tune}'
''').df()
```

This give us enough data to work with, even if we might like more test users.
For more thoroughness, let's look at how many test users we have by training
rating count:

```{python}
db.execute("""
WITH
    query_users AS (
        SELECT DISTINCT 'test' AS part, $test AS cutoff, user_id
        FROM ratings
        WHERE timestamp >= $test
        UNION ALL
        SELECT DISTINCT 'tune' AS part, $tune AS cutoff, user_id
        FROM ratings
        WHERE timestamp >= $tune AND timestamp < $test
    )
SELECT part, train_size, COUNT(DISTINCT user_id) AS user_count
FROM query_users q,
LATERAL (
    SELECT COUNT(*) AS train_size
    FROM ratings
    WHERE user_id = q.user_id
    AND timestamp < q.cutoff
) ts
GROUP BY part, train_size
""", {'test': date.fromisoformat(t_test), 'tune': date.fromisoformat(t_tune)})
train_sizes = db.fetchdf()
(
    pn.ggplot(train_sizes)
    + pn.aes(x='train_size', y='user_count', color='part')
    + pn.geom_line()
    + pn.scale_x_log10()
    + pn.xlab('Training Size')
    + pn.ylab('# of Users')
    + pn.ggtitle("Users by Training Size")
)
```

```{python}
tss = train_sizes.sort_values('train_size', ascending=False).groupby('part').apply(lambda df: df.assign(nc_uc=df['user_count'].cumsum()))
(
    pn.ggplot(tss[tss['train_size'] > 0])
    + pn.aes(x='train_size', y='nc_uc', color='part')
    + pn.geom_line()
    + pn.scale_x_log10()
    + pn.xlab('Training Size Cutoff')
    + pn.ylab('# of Test Users')
    + pn.ggtitle("Users Lost by Training Size")
)
```

Since we have very small loss up through 10â€“11 ratings, we will use all users who
appear at least once in training as our test users.

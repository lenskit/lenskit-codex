---
title: FlexMF Explicit on ML100K
echo: false
deps:
- sweeps/random/flexmf-explicit-random
---

This page analyzes the hyperparameter tuning results for the FlexMF scorer in
explicit-feedback mode (a biased matrix factorization model trained with
PyTorch).

```{python}
from codex.reporting.prelude import *
```

```{python}
runs = load_sweep_runs('flexmf-explicit', method='optuna')
iters = load_sweep_iters('flexmf-explicit', method='optuna')
result = load_sweep_result('flexmf-explicit', method='optuna')
best_id = result['trial_id']
```

## Parameter Search Space

```{python}
from codex.models.flexmf_explicit import SEARCH_SPACE
show_param_space(SEARCH_SPACE)
```

## Final Result

Searching selected the following configuration:

```{python}
rich.print(result['config'])
```

With these metrics:

```{python}
rich.print(result)
```

## Parameter Analysis

### Embedding Size

The embedding size is the hyperparameter that most affects the model's
fundamental logic, so let's look at performance as a fufnction of it:

```{python}
(
    pn.ggplot(runs)
    + pn.aes(x='config.embedding_size', y='RMSE')
    + pn.geom_point()
)
```

### Learning Parameters

::: {.panel-tabset}

#### Learning Rate

```{python}
(
    pn.ggplot(runs)
    + pn.aes(x='config.learning_rate', y='RMSE')
    + pn.geom_point()
    + pn.scale_x_log10()
)
```

#### Regularization

```{python}
(
    pn.ggplot(runs)
    + pn.aes(x='config.regularization', y='RMSE', color='config.reg_method', shape='config.reg_method')
    + pn.geom_point()
    + pn.scale_x_log10()
    + pn.labs(color='Reg. Method', shape='Reg. Method', x='Regularization')
)
```

:::

## Iteration Completion

How many iterations, on average, did we complete?

```{python}
(
    pn.ggplot(runs)
    + pn.aes(x='training_iteration')
    + pn.geom_histogram(binwidth=1)
)
```

How did the metric progress in the best result?

```{python}
best_iters = iters[iters['trial_id'] == best_id]
best_scores = best_iters.set_index('training_iteration')['RMSE']
med_scores = iters.groupby('training_iteration')['RMSE'].median()
iter_scores = pd.concat({'Best': best_scores, 'Median': med_scores}, names=['Run'])
iter_scores = iter_scores.reset_index()
(
    pn.ggplot(iter_scores)
    + pn.aes(x='training_iteration', y='RMSE', color='Run')
    + pn.geom_line()
)
```

How did the metric progress in the longest results?

```{python}
max_iter = iters['training_iteration'].max()
last_results = iters[iters['training_iteration'] == max_iter]
full_trials = last_results['trial_id']
full_iters = iters[iters['trial_id'].isin(full_trials)]
(
    pn.ggplot(full_iters)
    + pn.aes(x='training_iteration', y='RMSE', color='trial_id')
    + pn.geom_line()
    + pn.ggtitle('Performance of Full-Length Runs')
)
```

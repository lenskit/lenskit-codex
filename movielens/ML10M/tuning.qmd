---
title: Tuning Analysis
pagetitle: ML10M Tuning Analysis
order: 4
jupyter: python3
deps:
- sweeps/temporal/
---

This page provides an analysis of the hyperparameter sweeps to get a handle on
how different tuning methods are performing.  We use these results from early
data sets to select the most efficient strategies for other data sets.

All results are validation error.

```{python}
import sys

from codex.reporting.prelude import *
```

```{python}
names = ['model', 'search']
all_runs = {}
all_iters = {}
for model in DATA_INFO.models:
    for split in DATA_INFO.splits:
        for search in DATA_INFO.searches:
            try:
                all_runs[(model, search)] = load_sweep_runs(model, split=split, method=search)
            except FileNotFoundError as e:
                continue
            try:
                all_iters[(model,search)] = load_sweep_iters(model, split=split, method=search)
            except FileNotFoundError:
                pass
runs = pd.concat({
    k: df[['trial_id', 'timestamp', 'RBP', 'NDCG']
          + (['RMSE'] if 'RMSE' in df.columns else [])]
    for k, df in all_runs.items()
}, names=names)
runs = runs.reset_index(names).reset_index(drop=True)
runs = runs[runs['RBP'].notnull()]
iters = pd.concat({
    k: df[['trial_id', 'training_iteration', 'timestamp', 'time_this_iter_s', 'RBP', 'NDCG']
          + (['RMSE'] if 'RMSE' in df.columns else [])]
    for k, df in all_iters.items()
}, names=names)
iters = iters.reset_index(names).reset_index(drop=True)
```

## Trial Performance

These charts display the maximum performance so far as the search
progresses through its trials.  The vertical lines are at 60 trials.

```{python}
topn_runs = runs[runs['RMSE'].isnull()].copy().sort_values('timestamp')
topn_runs['TrialNum'] = topn_runs.groupby(
    ['model', 'search']
)['timestamp'].rank()
topn_runs['MaxRBP'] = topn_runs.groupby(
    ['model', 'search']
)['RBP'].cummax()
topn_runs['MaxNDCG'] = topn_runs.groupby(
    ['model', 'search']
)['NDCG'].cummax()
```

::: {.panel-tabset}

### RBP

```{python}
(
    pn.ggplot(topn_runs)
    + pn.aes(x='TrialNum', y='MaxRBP', color='model')
    + pn.geom_vline(xintercept=40, color='grey', linetype='dotted')
    + pn.geom_vline(xintercept=60, color='grey', linetype='dotted')
    + pn.geom_line(pn.aes(y='RBP'), alpha=0.2)
    + pn.geom_line()
    + pn.facet_grid('search')
)
```

### NDCG


```{python}
(
    pn.ggplot(topn_runs)
    + pn.aes(x='TrialNum', y='MaxNDCG', color='model')
    + pn.geom_vline(xintercept=40, color='grey', linetype='dotted')
    + pn.geom_vline(xintercept=60, color='grey', linetype='dotted')
    + pn.geom_line(pn.aes(y='NDCG'), alpha=0.2)
    + pn.geom_line()
    + pn.facet_grid('search')
)
```

::: {.callout-note}
Intelligent search methods (HyperOpt and Optuna) are searching for RBP,
so the NDCG performance may not be fully reflective.
:::

:::

## Loss

Our goal is to determine where to stop searching, and which methods are more
efficient at searching the space.  To better assess this, let's look at the _loss_ relative to each method's best performance if we stop at each trial point.

```{python}
max_rbp = topn_runs.groupby(
    ['model', 'search']
)['RBP'].max().to_frame('FinalBestRBP')
tnr = topn_runs.join(max_rbp, on=['model', 'search'])
tnr['RBPLoss'] = (tnr['FinalBestRBP'] - tnr['MaxRBP']) / tnr['FinalBestRBP']
(
    pn.ggplot(tnr)
    + pn.aes(x='TrialNum', y='RBPLoss', color='model')
    + pn.geom_vline(xintercept=40, color='grey', linetype='dotted')
    + pn.geom_vline(xintercept=60, color='grey', linetype='dotted')
    + pn.geom_hline(yintercept=0.01, color='grey', linetype='dashed')
    + pn.geom_hline(yintercept=0.05, color='pink', linetype='dashed')
    + pn.geom_line()
    + pn.scale_y_sqrt()
    + pn.facet_grid('search')
)
```

Loss drops pretty quick for intelligent searches, going under 5% by about 30
iterations on ML10M.

## Exploring Loss Loss

This is an interactive display so we can directly examine the results of stopping
the search at different trial counts.

```{python}
best_rbps = topn_runs.groupby(['model', 'search'])['RBP'].max().unstack()
ojs_define(
    ntrials=topn_runs['TrialNum'].max(),
    runs=topn_runs,
    best_rbps=best_rbps.reset_index(),
)
```


```{ojs}
viewof max_trial = Inputs.range(
  [20, ntrials],
  {value: ntrials, step: 5, label: "Maximum trials:"}
)
```

```{ojs}
filt_runs = transpose(runs).filter((run) => run.TrialNum <= max_trial)
best_list = transpose(best_rbps)
rbp_lookup = Object.fromEntries(best_list.map((m) => [m.model, m]))

function makeRunTable(cutoff) {
    let tbl = [];
    for (let mrec of best_list) {
        let row = {model: mrec.model};
        for (let search in mrec) {
            if (search == 'model') continue;

            let runs = filt_runs.filter((r) => r.model == mrec.model && r.search == search && r.TrialNum <= cutoff);
            let run = runs[runs.length - 1]
            if (run) {
                row[search] = `${run.MaxRBP.toFixed(3)} / ${mrec[search].toFixed(3)}`;
            }
        }
        tbl.push(row);
    }
    return tbl;
}

search_tbl = makeRunTable(max_trial);
Inputs.table(search_tbl)
```

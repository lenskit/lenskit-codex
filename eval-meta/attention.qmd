---
title: Rank-Biasing Attention Models
---

Rank-discounted metrics weigh recommended items by their ranking position using
a discount or *position attention* (or exposure) model approximating the amount
attention users pay to different ranking positions.  Some metrics have a model
inherent to their design, while others (such as RBP) have pluggable attention models.

This document describes the behavior of those attention models.


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```

```{python}
from codex.reporting.plots import DEFAULTS
```

## Core Attention Models

- Logarithmic discounting (typically base-2, used in NDCG)
- Reciprocal rank (used by MRR)
- Geometric cascade (default for RBP, which is reconfigurable)

```{python}
ranks = np.arange(10, dtype=np.float32) + 1
attn = [
    ('solid', r'$\log_2(max)$', np.reciprocal(np.log2(np.maximum(ranks, 2)))),
    ('solid', r'$\log_2(1+\cdot)$', np.reciprocal(np.log2(ranks + 1))),
    ('dotted', 'Recip', np.reciprocal(ranks)),
    ('dashed', r'$\mathrm{RBP}_{0.5}$', np.power(0.5, ranks - 1)),
    ('dashed', r'$\mathrm{RBP}_{0.2}$', np.power(0.2, ranks - 1)),
    ('dashed', r'$\mathrm{RBP}_{0.85}$', np.power(0.85, ranks - 1)),
    ((0, (1,2)), r'$\mathrm{HL}_5$', np.power(2, -(ranks - 1) / 4)),
]
for ls, label, ys in attn:
    plt.plot(ranks, ys, linestyle=ls, label=label)
plt.legend()
plt.show()
```

```{python}
ranks = np.arange(100, dtype=np.float32) + 1
attn = [
    ('solid', r'$\log_2(max)$', np.reciprocal(np.log2(np.maximum(ranks, 2)))),
    ('solid', r'$\log_2(1+\cdot)$', np.reciprocal(np.log2(ranks + 1))),
    ('dotted', 'Recip', np.reciprocal(ranks)),
    ('dashed', r'$\mathrm{RBP}_{0.5}$', np.power(0.5, ranks - 1)),
    ('dashed', r'$\mathrm{RBP}_{0.2}$', np.power(0.2, ranks - 1)),
    ('dashed', r'$\mathrm{RBP}_{0.85}$', np.power(0.85, ranks - 1)),
    ((0, (1,2)), r'$\mathrm{HL}_5$', np.power(2, -(ranks - 1) / 4)),
]
for ls, label, ys in attn:
    plt.plot(ranks, ys, linestyle=ls, label=label)
plt.legend()
plt.show()
```
